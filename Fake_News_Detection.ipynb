{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9wawsDIgxFw3"
      },
      "source": [
        "**DESCRIPTION** : A machine learning program to identify when an article might be fake news"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QlAPrg3SxdYi"
      },
      "source": [
        "**Dataset Description**\n",
        "\n",
        " *train.csv*: A full training dataset with the following attributes:\n",
        "\n",
        "    1. id: unique id for a news article\n",
        "    2. title: the title of a news article\n",
        "    3. author: author of the news article\n",
        "    4. text: the text of the article; could be incomplete\n",
        "    5. label: a label that marks the article as potentially unreliable\n",
        "                1: unreliable\n",
        "                0: reliable\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zcoYXkX9zBRZ"
      },
      "source": [
        "**Importing the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWMioV4DctvD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re #(regular expression - a library useful for searching word in a paragraph)\n",
        "from nltk.corpus import stopwords # stopword- words that doesn't add much value of the paragraph , like -'the' ,'with' ,'i',etc.\n",
        "from nltk.stem.porter import PorterStemmer # returns the root of a word by removing its prefix and suffix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # to convert text into feature vector (i.e numbers)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWQCD0lpYjeb"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj-1p-6_Y2Qs"
      },
      "outputs": [],
      "source": [
        "# printing the stopwords of English language\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_jVi9MAvcRP2"
      },
      "source": [
        "**Data Pre-pocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XhFAMV3ac_ms"
      },
      "outputs": [],
      "source": [
        "#Loading training dataset to pandas framework\n",
        "news_dataset = pd.read_csv('/dataset/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g5hth5DfcjZ"
      },
      "outputs": [],
      "source": [
        "# size of dataset\n",
        "news_dataset.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwbCgEiMfCdZ"
      },
      "outputs": [],
      "source": [
        "# printing the first 5 rows of the dataframe\n",
        "news_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0J6Hdu8pG5L"
      },
      "outputs": [],
      "source": [
        "news_dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b-5Vujilpme0"
      },
      "outputs": [],
      "source": [
        "# replacing the null values with empty string\n",
        "news_dataset = news_dataset.fillna('')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "etdtxkL1edkL"
      },
      "source": [
        "Now Let’s explore the unique values in the each category using below code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-iXbHqYeYYJ"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=news_dataset,\n",
        "              x='label',\n",
        "              order=news_dataset['label'].value_counts().index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_HHVhR9bARLr"
      },
      "outputs": [],
      "source": [
        "# Merging 'title' and 'author' column in a new column 'body'\n",
        "news_dataset['body']=news_dataset['author']+' '+news_dataset['title']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3h2oV0hA8Em"
      },
      "outputs": [],
      "source": [
        "print(news_dataset['body'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bolxswJwRkvc"
      },
      "source": [
        "**Stemming :**\n",
        "\n",
        "Returning only the root word by removing its prefix and suffix.\n",
        "\n",
        "For eg :\n",
        "teacher,teaching --> teach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A3FrpgGqRYw5"
      },
      "outputs": [],
      "source": [
        "port_stem =PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N99kpp59Sc60"
      },
      "outputs": [],
      "source": [
        "def stemming(body):\n",
        "  # replacing all the words(like number , punctuations etc) that do not come in the below defined set (i.e. contains only alphabet ) with a space\n",
        "  stemmed_body =re.sub('[^a-zA-Z]',' ',body)\n",
        "  # converting all words to lowercase,as it might happen that our model misinterpret upper letter word to be more significant\n",
        "  stemmed_body =stemmed_body.lower()\n",
        "  # splitting into list\n",
        "  stemmed_body =stemmed_body.split()\n",
        "  # stem all the words that are not stopwords\n",
        "  stemmed_body =[port_stem.stem(word) for word in stemmed_body if not word in stopwords.words('english')]\n",
        "  # join all the stemmed words\n",
        "  stemmed_body =' '.join(stemmed_body)\n",
        "  return stemmed_body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QiXg1R4FYMFO"
      },
      "outputs": [],
      "source": [
        "# Applying stemming function to our dataset's body column\n",
        "news_dataset['body']=news_dataset['body'].apply(stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VrwcAvgZnoU"
      },
      "outputs": [],
      "source": [
        "print(news_dataset['body'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gjL8ji7jZ1v1"
      },
      "outputs": [],
      "source": [
        "# Seperating the data and label\n",
        "X =news_dataset['body'].values\n",
        "Y =news_dataset['label'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7xeU3CswG3C"
      },
      "outputs": [],
      "source": [
        "print(X)\n",
        "print(Y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxn16I_ZejYy"
      },
      "source": [
        "Let’s visualize the WordCloud for fake and real news separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLr5xNXWdryX"
      },
      "outputs": [],
      "source": [
        "# Real\n",
        "consolidated = ' '.join(\n",
        "    word for word in news_dataset['body'][news_dataset['label'] == 0].astype(str))\n",
        "wordCloud = WordCloud(width=1600,\n",
        "                      height=800,\n",
        "                      random_state=21,\n",
        "                      max_font_size=110,\n",
        "                      collocations=False)\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW80B0bBfDjo"
      },
      "outputs": [],
      "source": [
        "# Fake\n",
        "consolidated = ' '.join(\n",
        "    word for word in news_dataset['body'][news_dataset['label'] == 1].astype(str))\n",
        "wordCloud = WordCloud(width=1600,\n",
        "                      height=800,\n",
        "                      random_state=21,\n",
        "                      max_font_size=110,\n",
        "                      collocations=False)\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mbDDRdGhxLAp"
      },
      "source": [
        "**What is a TfidfVectorizer?**\n",
        "\n",
        "*TF (Term Frequency):* The number of times a word appears in a document is its Term Frequency. A higher value means a term appears more often than others, and so, the document is a good match when the term is part of the search terms.\n",
        "\n",
        "*IDF (Inverse Document Frequency):* Words that occur many times a document, but also occur many times in many others, may be irrelevant. IDF is a measure of how significant a term is in the entire corpus.\n",
        "\n",
        "The TfidfVectorizer converts a collection of raw documents into a matrix of TF-IDF features.\n",
        "\n",
        "Thus in TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "B4bEAC3KwcX6"
      },
      "outputs": [],
      "source": [
        "# Converting the textual data to numerical data using TfidfVectorizer\n",
        "vectorizer =TfidfVectorizer()\n",
        "vectorizer.fit(X) # not doing on Y as it is already a numerical vector\n",
        "X =vectorizer.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIR8Vcxp17RI"
      },
      "outputs": [],
      "source": [
        "# X after numerical vectorization\n",
        "print(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SXlaHzFL2DNK"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset to training and test data\n",
        "X_train,X_test,Y_train,Y_test =train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iZgJIiA13GMd"
      },
      "source": [
        "**Note** : In the above cell\n",
        "\n",
        "*stratify=Y* will make sure that random split has same proportion of 0 's and 1 's as that in original dataset\n",
        "\n",
        "*random_state* is used to set the seed for the random generator so that we can ensure that the results that we get can be reproduced."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0_IogX_3xGH"
      },
      "source": [
        "**Model Training : logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTwPdpMF3wpA"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression()\n",
        "#training the model\n",
        "model.fit(X_train,Y_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u1LqL0LAULvd"
      },
      "source": [
        "**Evaluation and Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Q_xXDUgB3Che"
      },
      "outputs": [],
      "source": [
        "# Predict training data\n",
        "Y_train_pred = model.predict(X_train)\n",
        "# accuracy score of training data\n",
        "training_accuracy = accuracy_score(Y_train_pred,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK3ymTR92llZ"
      },
      "outputs": [],
      "source": [
        "print('Accuracy score of training data :',training_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2zB2TacNV78T"
      },
      "outputs": [],
      "source": [
        "# Predict testing data\n",
        "Y_test_pred = model.predict(X_test)\n",
        "# accuracy score of test data\n",
        "test_accuracy = accuracy_score(Y_test_pred,Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia44PJ1AWYrt"
      },
      "outputs": [],
      "source": [
        "print('Accuracy score of test data :',test_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4rglY8Hhgizi"
      },
      "source": [
        "**Model Training : Decision Tree Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQSRDdGmgiIT"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeClassifier()\n",
        "#training the model\n",
        "model.fit(X_train,Y_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m5xX0XDog4gK"
      },
      "source": [
        "**Evaluation and Prediction**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "AL2todhWg7Rr"
      },
      "outputs": [],
      "source": [
        "# Predicting training data\n",
        "Y_train_pred=model.predict(X_train)\n",
        "# accuracy score of training data\n",
        "training_accuracy = accuracy_score(Y_train_pred,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftvo0bEqhsSe",
        "outputId": "9c761544-9b7d-499d-82a3-a1cc7981297d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy score of training data : 1.0\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy score of training data :',training_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "wll9IVXbhwjI"
      },
      "outputs": [],
      "source": [
        "# Predict testing data\n",
        "Y_test_pred = model.predict(X_test)\n",
        "# accuracy score of test data\n",
        "test_accuracy = accuracy_score(Y_test_pred,Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcWf8vwoh2Ri"
      },
      "outputs": [],
      "source": [
        "print('Accuracy score of test data :',test_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IFg2M6Z8itjP"
      },
      "source": [
        "The confusion matrix for Decision Tree Classifier can be implemented with the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lSrX7z9iul3"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix of Results from Decision Tree classification\n",
        "cm = metrics.confusion_matrix(Y_test, model.predict(X_test))\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                                            display_labels=[False, True])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qkcOWkLLh-lA"
      },
      "source": [
        "**Making a predictive system**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eyuu0G6wh9DP"
      },
      "outputs": [],
      "source": [
        "X_new =X_test[0]\n",
        "prediction = model.predict(X_new)\n",
        "print(prediction)\n",
        "if (prediction[0]==0):\n",
        "  print(\"It's a real news\")\n",
        "else :\n",
        "  print(\"It's a fake news\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2NKXT60iC_7"
      },
      "outputs": [],
      "source": [
        "# Cross verifying above prediciton\n",
        "print(Y_test[0])\n",
        "if (Y_test[0]==0):\n",
        "  print(\"It's a real news\")\n",
        "else :\n",
        "  print(\"It's a fake news\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zrpw20pjPHh"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "*Decision Tree Classifier* and *Logistic regression* are performing well."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
